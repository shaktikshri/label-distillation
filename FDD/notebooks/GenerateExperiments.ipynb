{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, glob\n",
    "import numpy as np\n",
    "\n",
    "experiment_results_directory_name = 'results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"batch_size\": 1024,\n",
    "    \"epochs\": 10,\n",
    "    \"baseline\": False,\n",
    "    \"num_base_examples\": 20,\n",
    "    \"target_set_size\": 50000,\n",
    "    \"num_inner_steps\": 10,\n",
    "    \"inner_batch_size\": 10,\n",
    "    \"expname\": \"expname\",\n",
    "    \"source\": \"emnist\",\n",
    "    \"target\": \"mnist\",\n",
    "    \"balanced_source\": True,\n",
    "    \"resnet\": False,\n",
    "    \"inner_lr\": 0.01,\n",
    "    \"random_seed\": 1234,\n",
    "    \"num_steps_analysis\": False,\n",
    "    \"test_various_models\": False,\n",
    "    \"label_smoothing\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cfg(cfg):\n",
    "    experiment_name = cfg['expname']\n",
    "    with open(os.path.join('..', 'experiment_configs', experiment_name + '.json'), 'w') as f:\n",
    "        json.dump(cfg, f, sort_keys=True)\n",
    "        \n",
    "    with open(os.path.join('..', 'experiment_configs', 'configs.txt'), 'a') as f:\n",
    "        f.write(experiment_name + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_name(cfg, method=\"svm\", ld=True):\n",
    "    if ld:\n",
    "        experiment_name = \"ld_{}be_{}ib\".format(cfg['num_base_examples'], cfg['inner_batch_size'])\n",
    "    else:\n",
    "        experiment_name = \"dd_{}be_{}ib\".format(cfg['num_base_examples'], cfg['inner_batch_size'])\n",
    "\n",
    "    if cfg['target_set_size'] not in [45000, 50000]:\n",
    "        experiment_name += \"_{}ts\".format(cfg['target_set_size'])\n",
    "    \n",
    "    dataset_map = {\"emnist\": \"emn\", \"mnist\": \"mn\", \"fake\": \"fake\", \"cub\": \"cub\",\n",
    "                   \"svhn\": \"svhn\", \"cifar10\": \"c10\", \"kmnist\": \"km\", \"imagenet\": \"imn\", \"cifar100\": \"c100\", \"k49\": \"k49\"}\n",
    "    \n",
    "    if cfg['baseline']:\n",
    "        experiment_name += \"_baseline\"\n",
    "    experiment_name += \"_\" + dataset_map[cfg['source']] + \"_to_\" + dataset_map[cfg['target']]\n",
    "\n",
    "    if cfg['balanced_source']:\n",
    "        experiment_name += \"_bl\"\n",
    "    else:\n",
    "        experiment_name += \"_ubl\"\n",
    "\n",
    "    if cfg['label_smoothing']:\n",
    "        experiment_name += \"_ls\"\n",
    "          \n",
    "    if cfg['resnet']:\n",
    "        experiment_name += \"_rn\"\n",
    "    else:    \n",
    "        experiment_name += \"_cnn\"\n",
    "\n",
    "    if not cfg['baseline']:\n",
    "        experiment_name += \"_{}e\".format(cfg['epochs'])\n",
    "    \n",
    "    if cfg['num_steps_analysis']:\n",
    "        experiment_name += \"_nsa\"\n",
    "\n",
    "    if cfg['test_various_models']:\n",
    "        experiment_name += \"_tvm\"\n",
    "    \n",
    "    experiment_name += \"_\" + method\n",
    "    experiment_name += \"_\" + str(cfg['random_seed']) + \"s\"\n",
    "    \n",
    "    experiment_name += \"_v\"\n",
    "    \n",
    "    experiment_configs_pattern = os.path.join('..', 'experiment_configs', experiment_name + '*')\n",
    "    previous_experiment_configs = glob.glob(experiment_configs_pattern)\n",
    "    \n",
    "    version_num = 0\n",
    "        \n",
    "    if previous_experiment_configs:\n",
    "        version_num_cfgs = max(sorted(map(lambda experiment_name: int(experiment_name[len(experiment_configs_pattern)-1:-5]), previous_experiment_configs))) + 1\n",
    "        if version_num_cfgs > version_num:\n",
    "            version_num = version_num_cfgs\n",
    "                   \n",
    "    experiment_name = experiment_name + str(version_num)\n",
    "    \n",
    "    return experiment_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ld_create_experiments_2(cfg):\n",
    "    cfg['test_various_models'] = False\n",
    "    cfg['baseline'] = False\n",
    "    cfg['epochs'] = 400\n",
    "    cfg['balanced_source'] = True\n",
    "    cfg['resnet'] = False\n",
    "    cfg['inner_lr'] = 0.01\n",
    "    cfg['random_seed'] = 1234\n",
    "    cfg['num_steps_analysis'] = False\n",
    "    cfg['batch_size'] = 1024\n",
    "\n",
    "    # LD MNIST\n",
    "    cfg['source'] = \"mnist\"\n",
    "    cfg['target'] = \"mnist\"\n",
    "    ld_create_experiments_1(cfg) \n",
    "\n",
    "    # LD CIFAR-10\n",
    "    cfg['source'] = \"cifar10\"\n",
    "    cfg['target'] = \"cifar10\"\n",
    "    ld_create_experiments_1(cfg)\n",
    "\n",
    "    # baselines without label smoothing\n",
    "    cfg['baseline'] = True\n",
    "    # # MNIST Baseline\n",
    "    cfg['source'] = \"mnist\"\n",
    "    cfg['target'] = \"mnist\"\n",
    "    ld_create_experiments_1(cfg)\n",
    "\n",
    "    # # CIFAR-10 Baseline\n",
    "    cfg['source'] = \"cifar10\"\n",
    "    cfg['target'] = \"cifar10\"\n",
    "    ld_create_experiments_1(cfg) \n",
    "\n",
    "    cfg['baseline'] = False\n",
    "\n",
    "    # baselines with label smoothing\n",
    "    cfg['baseline'] = True\n",
    "    # # MNIST Baseline\n",
    "    cfg['label_smoothing'] = 0.1\n",
    "    cfg['source'] = \"mnist\"\n",
    "    cfg['target'] = \"mnist\"\n",
    "    ld_create_experiments_1(cfg)\n",
    "\n",
    "    # # CIFAR-10 Baseline\n",
    "    cfg['source'] = \"cifar10\"\n",
    "    cfg['target'] = \"cifar10\"\n",
    "    ld_create_experiments_1(cfg) \n",
    "\n",
    "    cfg['baseline'] = False\n",
    "    cfg['label_smoothing'] = 0\n",
    "\n",
    "    # for cross-dataset cases, the source does not need to be balanced\n",
    "    cfg['balanced_source'] = False\n",
    "\n",
    "    # LD E to M\n",
    "    cfg['source'] = \"emnist\"\n",
    "    cfg['target'] = \"mnist\"\n",
    "    ld_create_experiments_1(cfg)\n",
    "\n",
    "    # LD E to K\n",
    "    cfg['source'] = \"emnist\"\n",
    "    cfg['target'] = \"kmnist\"\n",
    "    ld_create_experiments_1(cfg)\n",
    "\n",
    "    # LD B to C\n",
    "    cfg['source'] = \"cub\"\n",
    "    cfg['target'] = \"cifar10\"\n",
    "    ld_create_experiments_1(cfg)\n",
    "\n",
    "    # LD E to K49\n",
    "    cfg['source'] = \"emnist\"\n",
    "    cfg['target'] = \"k49\"\n",
    "    ld_create_experiments_1(cfg)\n",
    "\n",
    "    cfg['balanced_source'] = True\n",
    "\n",
    "\n",
    "def ld_create_experiments_1(cfg):\n",
    "    be_list = [10, 20, 50, 100, 200, 500]\n",
    "    for be_num in be_list:\n",
    "        cfg['num_base_examples'] = be_num\n",
    "        if cfg['num_base_examples'] == 10:\n",
    "            cfg['inner_batch_size'] = 10\n",
    "        elif cfg['num_base_examples'] == 20:\n",
    "            cfg['inner_batch_size'] = 10\n",
    "        elif cfg['num_base_examples'] == 50:\n",
    "            cfg['inner_batch_size'] = 25\n",
    "        elif cfg['num_base_examples'] == 100:\n",
    "            cfg['inner_batch_size'] = 50\n",
    "        else:\n",
    "            cfg['inner_batch_size'] = 50\n",
    "\n",
    "        if cfg['target'] == \"k49\":\n",
    "            if cfg['num_base_examples'] > 100:\n",
    "                cfg['epochs'] = 200\n",
    "            else:\n",
    "                cfg['epochs'] = 100\n",
    "        else:\n",
    "            if cfg['num_base_examples'] > 100:\n",
    "                cfg['epochs'] = 800\n",
    "            else:\n",
    "                cfg['epochs'] = 400\n",
    "        \n",
    "        cfg['expname'] = get_experiment_name(cfg, method=\"or2\")  # choose or2 or rr\n",
    "        print(cfg['expname'])\n",
    "        save_cfg(cfg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ld_10be_10ib_mn_to_mn_bl_cnn_400e_or2_1234s_v0\nld_20be_10ib_mn_to_mn_bl_cnn_400e_or2_1234s_v0\nld_50be_25ib_mn_to_mn_bl_cnn_400e_or2_1234s_v0\nld_100be_50ib_mn_to_mn_bl_cnn_400e_or2_1234s_v0\nld_200be_50ib_mn_to_mn_bl_cnn_800e_or2_1234s_v0\nld_500be_50ib_mn_to_mn_bl_cnn_800e_or2_1234s_v0\nld_10be_10ib_c10_to_c10_bl_cnn_400e_or2_1234s_v0\nld_20be_10ib_c10_to_c10_bl_cnn_400e_or2_1234s_v0\nld_50be_25ib_c10_to_c10_bl_cnn_400e_or2_1234s_v0\nld_100be_50ib_c10_to_c10_bl_cnn_400e_or2_1234s_v0\nld_200be_50ib_c10_to_c10_bl_cnn_800e_or2_1234s_v0\nld_500be_50ib_c10_to_c10_bl_cnn_800e_or2_1234s_v0\nld_10be_10ib_baseline_mn_to_mn_bl_cnn_or2_1234s_v0\nld_20be_10ib_baseline_mn_to_mn_bl_cnn_or2_1234s_v0\nld_50be_25ib_baseline_mn_to_mn_bl_cnn_or2_1234s_v0\nld_100be_50ib_baseline_mn_to_mn_bl_cnn_or2_1234s_v0\nld_200be_50ib_baseline_mn_to_mn_bl_cnn_or2_1234s_v0\nld_500be_50ib_baseline_mn_to_mn_bl_cnn_or2_1234s_v0\nld_10be_10ib_baseline_c10_to_c10_bl_cnn_or2_1234s_v0\nld_20be_10ib_baseline_c10_to_c10_bl_cnn_or2_1234s_v0\nld_50be_25ib_baseline_c10_to_c10_bl_cnn_or2_1234s_v0\nld_100be_50ib_baseline_c10_to_c10_bl_cnn_or2_1234s_v0\nld_200be_50ib_baseline_c10_to_c10_bl_cnn_or2_1234s_v0\nld_500be_50ib_baseline_c10_to_c10_bl_cnn_or2_1234s_v0\nld_10be_10ib_baseline_mn_to_mn_bl_ls_cnn_or2_1234s_v0\nld_20be_10ib_baseline_mn_to_mn_bl_ls_cnn_or2_1234s_v0\nld_50be_25ib_baseline_mn_to_mn_bl_ls_cnn_or2_1234s_v0\nld_100be_50ib_baseline_mn_to_mn_bl_ls_cnn_or2_1234s_v0\nld_200be_50ib_baseline_mn_to_mn_bl_ls_cnn_or2_1234s_v0\nld_500be_50ib_baseline_mn_to_mn_bl_ls_cnn_or2_1234s_v0\nld_10be_10ib_baseline_c10_to_c10_bl_ls_cnn_or2_1234s_v0\nld_20be_10ib_baseline_c10_to_c10_bl_ls_cnn_or2_1234s_v0\nld_50be_25ib_baseline_c10_to_c10_bl_ls_cnn_or2_1234s_v0\nld_100be_50ib_baseline_c10_to_c10_bl_ls_cnn_or2_1234s_v0\nld_200be_50ib_baseline_c10_to_c10_bl_ls_cnn_or2_1234s_v0\nld_500be_50ib_baseline_c10_to_c10_bl_ls_cnn_or2_1234s_v0\nld_10be_10ib_emn_to_mn_ubl_cnn_400e_or2_1234s_v0\nld_20be_10ib_emn_to_mn_ubl_cnn_400e_or2_1234s_v0\nld_50be_25ib_emn_to_mn_ubl_cnn_400e_or2_1234s_v0\nld_100be_50ib_emn_to_mn_ubl_cnn_400e_or2_1234s_v0\nld_200be_50ib_emn_to_mn_ubl_cnn_800e_or2_1234s_v0\nld_500be_50ib_emn_to_mn_ubl_cnn_800e_or2_1234s_v0\nld_10be_10ib_emn_to_km_ubl_cnn_400e_or2_1234s_v0\nld_20be_10ib_emn_to_km_ubl_cnn_400e_or2_1234s_v0\nld_50be_25ib_emn_to_km_ubl_cnn_400e_or2_1234s_v0\nld_100be_50ib_emn_to_km_ubl_cnn_400e_or2_1234s_v0\nld_200be_50ib_emn_to_km_ubl_cnn_800e_or2_1234s_v0\nld_500be_50ib_emn_to_km_ubl_cnn_800e_or2_1234s_v0\nld_10be_10ib_cub_to_c10_ubl_cnn_400e_or2_1234s_v0\nld_20be_10ib_cub_to_c10_ubl_cnn_400e_or2_1234s_v0\nld_50be_25ib_cub_to_c10_ubl_cnn_400e_or2_1234s_v0\nld_100be_50ib_cub_to_c10_ubl_cnn_400e_or2_1234s_v0\nld_200be_50ib_cub_to_c10_ubl_cnn_800e_or2_1234s_v0\nld_500be_50ib_cub_to_c10_ubl_cnn_800e_or2_1234s_v0\nld_10be_10ib_emn_to_k49_ubl_cnn_100e_or2_1234s_v0\nld_20be_10ib_emn_to_k49_ubl_cnn_100e_or2_1234s_v0\nld_50be_25ib_emn_to_k49_ubl_cnn_100e_or2_1234s_v0\nld_100be_50ib_emn_to_k49_ubl_cnn_100e_or2_1234s_v0\nld_200be_50ib_emn_to_k49_ubl_cnn_200e_or2_1234s_v0\nld_500be_50ib_emn_to_k49_ubl_cnn_200e_or2_1234s_v0\n"
    }
   ],
   "source": [
    "ld_create_experiments_2(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c100_create_experiments_2(cfg):\n",
    "    cfg['test_various_models'] = False\n",
    "    cfg['baseline'] = False\n",
    "    cfg['epochs'] = 800\n",
    "    cfg['balanced_source'] = True\n",
    "    cfg['resnet'] = False\n",
    "    cfg['inner_lr'] = 0.01\n",
    "    cfg['random_seed'] = 1234\n",
    "    cfg['num_steps_analysis'] = False\n",
    "    cfg['batch_size'] = 1024\n",
    "\n",
    "    # LD CIFAR-100\n",
    "    cfg['source'] = \"cifar100\"\n",
    "    cfg['target'] = \"cifar100\"\n",
    "    c100_create_experiments_1(cfg)\n",
    "\n",
    "    # also do baselines\n",
    "    cfg['baseline'] = True\n",
    "\n",
    "    cfg['source'] = \"cifar100\"\n",
    "    cfg['target'] = \"cifar100\"\n",
    "    c100_create_experiments_1(cfg)\n",
    "\n",
    "    cfg['label_smoothing'] = 0.1\n",
    "    cfg['source'] = \"cifar100\"\n",
    "    cfg['target'] = \"cifar100\"\n",
    "    c100_create_experiments_1(cfg)\n",
    "\n",
    "    cfg['baseline'] = False\n",
    "    cfg['label_smoothing'] = 0\n",
    "\n",
    "\n",
    "def c100_create_experiments_1(cfg):\n",
    "    cfg['num_base_examples'] = 100\n",
    "    cfg['inner_batch_size'] = 100\n",
    "    cfg['expname'] = get_experiment_name(cfg, method=\"or2\")  # choose or2 or rr\n",
    "    print(cfg['expname'])\n",
    "    save_cfg(cfg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ld_100be_100ib_c100_to_c100_bl_cnn_800e_or2_1234s_v0\nld_100be_100ib_baseline_c100_to_c100_bl_cnn_or2_1234s_v0\nld_100be_100ib_baseline_c100_to_c100_bl_ls_cnn_or2_1234s_v0\n"
    }
   ],
   "source": [
    "c100_create_experiments_2(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ld_analysis_create_experiments_2(cfg):\n",
    "    cfg['test_various_models'] = False\n",
    "    cfg['baseline'] = False\n",
    "    cfg['epochs'] = 400\n",
    "    cfg['balanced_source'] = True\n",
    "    cfg['resnet'] = False\n",
    "    cfg['inner_lr'] = 0.01\n",
    "    cfg['random_seed'] = 1234\n",
    "    cfg['num_steps_analysis'] = False\n",
    "    cfg['batch_size'] = 1024\n",
    "\n",
    "    # Analysis of using various base examples\n",
    "    # use 5 random seeds, do LD MNIST 100 base examples with this and also LD E to M.\n",
    "    random_seeds = [0, 465, 1234, 5439, 89432]\n",
    "    for random_seed in random_seeds:\n",
    "        cfg['random_seed'] = random_seed\n",
    "        cfg['source'] = \"mnist\"\n",
    "        cfg['target'] = \"mnist\"\n",
    "        ld_analysis_create_experiment(cfg) \n",
    "\n",
    "        cfg['balanced_source'] = False\n",
    "        cfg['source'] = \"emnist\"\n",
    "        cfg['target'] = \"mnist\"\n",
    "        ld_analysis_create_experiment(cfg) \n",
    "        cfg['balanced_source'] = True\n",
    "\n",
    "    cfg['random_seed'] = 1234\n",
    "\n",
    "    # Analysis of variable number of examples from the target set\n",
    "    # this makes sense only for the cross dataset version, so do it just with LD E to M 100 base examples\n",
    "    # Try the following numbers: [100, 500, 1000, 5000, 10000, 20000, all]\n",
    "    ts_num_list = [100, 500, 1000, 5000, 10000, 20000]\n",
    "    cfg['balanced_source'] = False\n",
    "    for ts_num in ts_num_list:\n",
    "        cfg['target_set_size'] = ts_num\n",
    "        cfg['source'] = \"emnist\"\n",
    "        cfg['target'] = \"mnist\"\n",
    "        ld_analysis_create_experiment(cfg) \n",
    "    \n",
    "    cfg['balanced_source'] = True\n",
    "    cfg['target_set_size'] = 50000\n",
    "\n",
    "    # Analysis of the impact of using different numbers of steps for training a test model\n",
    "    # LD MNIST 100 base examples with this and also LD E to M.\n",
    "    cfg['num_steps_analysis'] = True\n",
    "    cfg['source'] = \"mnist\"\n",
    "    cfg['target'] = \"mnist\"\n",
    "    ld_analysis_create_experiment(cfg) \n",
    "\n",
    "    cfg['balanced_source'] = False\n",
    "    cfg['source'] = \"emnist\"\n",
    "    cfg['target'] = \"mnist\"\n",
    "    ld_analysis_create_experiment(cfg)  \n",
    "    cfg['balanced_source'] = True\n",
    "\n",
    "    cfg['num_steps_analysis'] = False\n",
    "    \n",
    "\n",
    "def ld_analysis_create_experiment(cfg):\n",
    "    cfg['num_base_examples'] = 100\n",
    "    cfg['inner_batch_size'] = 50\n",
    "    cfg['expname'] = get_experiment_name(cfg, method=\"or2\")  # choose or2 or rr\n",
    "    print(cfg['expname'])\n",
    "    save_cfg(cfg) \n",
    "\n",
    "\n",
    "def ld_arch_analysis_create_experiments_2(cfg):\n",
    "    cfg['test_various_models'] = False\n",
    "    cfg['baseline'] = False\n",
    "    cfg['epochs'] = 400\n",
    "    cfg['balanced_source'] = True\n",
    "    cfg['resnet'] = False\n",
    "    cfg['inner_lr'] = 0.01\n",
    "    cfg['random_seed'] = 1234\n",
    "    cfg['num_steps_analysis'] = False\n",
    "    cfg['batch_size'] = 1024\n",
    "\n",
    "    cfg['test_various_models'] = True\n",
    "\n",
    "    # LD CIFAR-10\n",
    "    cfg['source'] = \"cifar10\"\n",
    "    cfg['target'] = \"cifar10\"\n",
    "    ld_arch_analysis_create_experiment(cfg) \n",
    "\n",
    "    cfg['baseline'] = True\n",
    "\n",
    "    # CIFAR-10 Baseline\n",
    "    cfg['source'] = \"cifar10\"\n",
    "    cfg['target'] = \"cifar10\"\n",
    "    ld_arch_analysis_create_experiment(cfg) \n",
    "\n",
    "    cfg['baseline'] = False\n",
    "    # for cross-dataset cases, the source does not need to be balanced\n",
    "    cfg['balanced_source'] = False\n",
    "\n",
    "    # LD B to C\n",
    "    cfg['source'] = \"cub\"\n",
    "    cfg['target'] = \"cifar10\"\n",
    "    ld_arch_analysis_create_experiment(cfg)\n",
    "\n",
    "    cfg['balanced_source'] = True\n",
    "\n",
    "    cfg['test_various_models'] = False\n",
    "\n",
    "\n",
    "def ld_arch_analysis_create_experiment(cfg):\n",
    "    be_list = [10, 20, 50, 100, 200, 500]\n",
    "    for be_num in be_list:\n",
    "        cfg['num_base_examples'] = be_num\n",
    "        if cfg['num_base_examples'] == 10:\n",
    "            cfg['inner_batch_size'] = 10\n",
    "        elif cfg['num_base_examples'] == 20:\n",
    "            cfg['inner_batch_size'] = 10\n",
    "        elif cfg['num_base_examples'] == 50:\n",
    "            cfg['inner_batch_size'] = 25\n",
    "        elif cfg['num_base_examples'] == 100:\n",
    "            cfg['inner_batch_size'] = 50\n",
    "        else:\n",
    "            cfg['inner_batch_size'] = 50\n",
    "        \n",
    "        cfg['expname'] = get_experiment_name(cfg, method=\"or2\")  # choose or2 or rr\n",
    "        print(cfg['expname'])\n",
    "        save_cfg(cfg) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ld_100be_50ib_mn_to_mn_bl_cnn_400e_rr_0s_v0\nld_100be_50ib_emn_to_mn_ubl_cnn_400e_rr_0s_v0\nld_100be_50ib_mn_to_mn_bl_cnn_400e_rr_465s_v0\nld_100be_50ib_emn_to_mn_ubl_cnn_400e_rr_465s_v0\nld_100be_50ib_mn_to_mn_bl_cnn_400e_rr_1234s_v1\nld_100be_50ib_emn_to_mn_ubl_cnn_400e_rr_1234s_v1\nld_100be_50ib_mn_to_mn_bl_cnn_400e_rr_5439s_v0\nld_100be_50ib_emn_to_mn_ubl_cnn_400e_rr_5439s_v0\nld_100be_50ib_mn_to_mn_bl_cnn_400e_rr_89432s_v0\nld_100be_50ib_emn_to_mn_ubl_cnn_400e_rr_89432s_v0\nld_100be_50ib_100ts_emn_to_mn_ubl_cnn_400e_rr_1234s_v0\nld_100be_50ib_500ts_emn_to_mn_ubl_cnn_400e_rr_1234s_v0\nld_100be_50ib_1000ts_emn_to_mn_ubl_cnn_400e_rr_1234s_v0\nld_100be_50ib_5000ts_emn_to_mn_ubl_cnn_400e_rr_1234s_v0\nld_100be_50ib_10000ts_emn_to_mn_ubl_cnn_400e_rr_1234s_v0\nld_100be_50ib_20000ts_emn_to_mn_ubl_cnn_400e_rr_1234s_v0\nld_100be_50ib_mn_to_mn_bl_cnn_400e_nsa_rr_1234s_v0\nld_100be_50ib_emn_to_mn_ubl_cnn_400e_nsa_rr_1234s_v0\n"
    }
   ],
   "source": [
    "ld_analysis_create_experiments_2(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ld_10be_10ib_c10_to_c10_bl_cnn_400e_tvm_or2_1234s_v0\nld_20be_10ib_c10_to_c10_bl_cnn_400e_tvm_or2_1234s_v0\nld_50be_25ib_c10_to_c10_bl_cnn_400e_tvm_or2_1234s_v0\nld_100be_50ib_c10_to_c10_bl_cnn_400e_tvm_or2_1234s_v0\nld_200be_50ib_c10_to_c10_bl_cnn_400e_tvm_or2_1234s_v0\nld_500be_50ib_c10_to_c10_bl_cnn_400e_tvm_or2_1234s_v0\nld_10be_10ib_baseline_c10_to_c10_bl_cnn_tvm_or2_1234s_v0\nld_20be_10ib_baseline_c10_to_c10_bl_cnn_tvm_or2_1234s_v0\nld_50be_25ib_baseline_c10_to_c10_bl_cnn_tvm_or2_1234s_v0\nld_100be_50ib_baseline_c10_to_c10_bl_cnn_tvm_or2_1234s_v0\nld_200be_50ib_baseline_c10_to_c10_bl_cnn_tvm_or2_1234s_v0\nld_500be_50ib_baseline_c10_to_c10_bl_cnn_tvm_or2_1234s_v0\nld_10be_10ib_cub_to_c10_ubl_cnn_400e_tvm_or2_1234s_v0\nld_20be_10ib_cub_to_c10_ubl_cnn_400e_tvm_or2_1234s_v0\nld_50be_25ib_cub_to_c10_ubl_cnn_400e_tvm_or2_1234s_v0\nld_100be_50ib_cub_to_c10_ubl_cnn_400e_tvm_or2_1234s_v0\nld_200be_50ib_cub_to_c10_ubl_cnn_400e_tvm_or2_1234s_v0\nld_500be_50ib_cub_to_c10_ubl_cnn_400e_tvm_or2_1234s_v0\n"
    }
   ],
   "source": [
    "ld_arch_analysis_create_experiments_2(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset distillation\n",
    "Change the name for this first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dd_create_experiments_2(cfg):\n",
    "    cfg['test_various_models'] = False\n",
    "    cfg['baseline'] = False\n",
    "    cfg['epochs'] = 400\n",
    "    cfg['balanced_source'] = True\n",
    "    cfg['resnet'] = False\n",
    "    cfg['inner_lr'] = 0.01\n",
    "    cfg['random_seed'] = 1234\n",
    "    cfg['num_steps_analysis'] = False\n",
    "    cfg['batch_size'] = 1024\n",
    "\n",
    "    # DD MNIST - 10, 20, 50 and 100 base examples\n",
    "    cfg['source'] = \"mnist\"\n",
    "    cfg['target'] = \"mnist\"\n",
    "    dd_create_experiment(cfg) \n",
    "\n",
    "    # DD CIFAR-10 - 10, 20, 50 and 100 base examples\n",
    "    cfg['source'] = \"cifar10\"\n",
    "    cfg['target'] = \"cifar10\"\n",
    "    dd_create_experiment(cfg)\n",
    "\n",
    "def dd_create_experiment(cfg):\n",
    "    be_list = [10, 20, 50, 100]\n",
    "    for be_num in be_list:\n",
    "        cfg['num_base_examples'] = be_num\n",
    "        if cfg['num_base_examples'] == 10:\n",
    "            cfg['inner_batch_size'] = 10\n",
    "        elif cfg['num_base_examples'] == 20:\n",
    "            cfg['inner_batch_size'] = 10\n",
    "        elif cfg['num_base_examples'] == 50:\n",
    "            cfg['inner_batch_size'] = 25\n",
    "        elif cfg['num_base_examples'] == 100:\n",
    "            cfg['inner_batch_size'] = 50\n",
    "        else:\n",
    "            cfg['inner_batch_size'] = 50\n",
    "        \n",
    "        cfg['expname'] = get_experiment_name(cfg, method=\"or2\", ld=False)  # choose or2 or rr\n",
    "        print(cfg['expname'])\n",
    "        save_cfg(cfg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dd_10be_10ib_mn_to_mn_bl_cnn_400e_or2_1234s_v0\ndd_20be_10ib_mn_to_mn_bl_cnn_400e_or2_1234s_v0\ndd_50be_25ib_mn_to_mn_bl_cnn_400e_or2_1234s_v0\ndd_100be_50ib_mn_to_mn_bl_cnn_400e_or2_1234s_v0\ndd_10be_10ib_c10_to_c10_bl_cnn_400e_or2_1234s_v0\ndd_20be_10ib_c10_to_c10_bl_cnn_400e_or2_1234s_v0\ndd_50be_25ib_c10_to_c10_bl_cnn_400e_or2_1234s_v0\ndd_100be_50ib_c10_to_c10_bl_cnn_400e_or2_1234s_v0\n"
    }
   ],
   "source": [
    "dd_create_experiments_2(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}